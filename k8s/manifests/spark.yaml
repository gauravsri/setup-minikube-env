---
# Spark on Kubernetes - RBAC Configuration
# This replaces the standalone Spark cluster with dynamic pod-based execution
# Pods are created on-demand via spark-submit with --master k8s://
# No persistent Spark master/worker/history deployments needed

# ServiceAccount for Spark driver and executor pods
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark
  labels:
    app: spark

---
# Role with permissions for Spark on Kubernetes
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: spark-role
  labels:
    app: spark
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps"]
  verbs: ["create", "get", "list", "watch", "delete", "deletecollection", "patch", "update"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list"]

---
# RoleBinding to bind spark ServiceAccount to spark-role
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: spark-role-binding
  labels:
    app: spark
subjects:
- kind: ServiceAccount
  name: spark
roleRef:
  kind: Role
  name: spark-role
  apiGroup: rbac.authorization.k8s.io

---
# PersistentVolume for project files (JAR access)
# Uses hostPath - requires minikube mount to be active
# IMPORTANT: Update 'path' to match your project directory
apiVersion: v1
kind: PersistentVolume
metadata:
  name: spark-project-pv
  labels:
    app: spark
    type: local
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /path/to/your/project  # UPDATE THIS: e.g., /Users/username/workspace/project
    type: DirectoryOrCreate
  persistentVolumeReclaimPolicy: Retain
  storageClassName: standard

---
# PersistentVolumeClaim for project files
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: spark-project-pvc
  labels:
    app: spark
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: standard
  selector:
    matchLabels:
      app: spark
      type: local

---
# USAGE INSTRUCTIONS
#
# Spark on Kubernetes runs jobs dynamically without persistent cluster.
# Pods are created on-demand when you submit jobs via spark-submit.
#
# Basic spark-submit example:
#
# kubectl run spark-job --rm -i --tty --restart=Never \
#   --namespace=<your-namespace> \
#   --serviceaccount=spark \
#   --image=apache/spark:3.5.3 \
#   -- /opt/spark/bin/spark-submit \
#      --master k8s://https://kubernetes.default.svc \
#      --deploy-mode cluster \
#      --name MySparkJob \
#      --class com.example.MyJob \
#      --conf spark.kubernetes.namespace=<your-namespace> \
#      --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
#      --conf spark.kubernetes.container.image=apache/spark:3.5.3 \
#      --conf spark.executor.instances=2 \
#      --conf spark.executor.memory=1g \
#      local:///project/target/my-app.jar
#
# Integration with Airflow:
#
# Use KubernetesPodOperator instead of SparkSubmitOperator:
#
# from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
#
# spark_task = KubernetesPodOperator(
#     task_id='run_spark_job',
#     name='spark-submitter',
#     namespace='<your-namespace>',
#     service_account_name='spark',
#     image='apache/spark:3.5.3',
#     cmds=['/bin/sh', '-c'],
#     arguments=['spark-submit --master k8s://... local:///project/target/app.jar']
# )
#
# Benefits over standalone Spark cluster:
# - No persistent pods consuming resources (saves ~6Gi memory)
# - Kubernetes-native pod lifecycle management
# - Better integration with RBAC and namespaces
# - Dynamic scaling based on workload
# - Simpler deployment and maintenance
#
# Prerequisites:
# - minikube mount for hostPath PV access (if using local files)
# - Proper RBAC configuration (included above)
# - Container image with your application JARs
#
# Monitoring:
# - kubectl get pods -n <namespace> | grep spark  # View running jobs
# - kubectl logs -n <namespace> <driver-pod> -f   # Follow driver logs
# - kubectl describe pod -n <namespace> <pod>     # Pod details
#
